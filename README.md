# ğŸš Urban Mobility Data Pipeline â€” Belo Horizonte

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um pipeline de dados ponta a ponta utilizando arquitetura Medallion (Bronze, Silver e Gold) para ingestÃ£o, processamento e anÃ¡lise de dados pÃºblicos de mobilidade urbana da cidade de Belo Horizonte.

O objetivo Ã© coletar dados de posicionamento em tempo real dos Ã´nibus e dados histÃ³ricos do Mapa de Controle Operacional (MCO), armazenÃ¡-los em um data lake, realizar transformaÃ§Ãµes e disponibilizar tabelas analÃ­ticas prontas para consumo por ferramentas de BI ou modelos de Machine Learning.

O pipeline foi desenvolvido utilizando Python e PySpark em ambiente local, seguindo boas prÃ¡ticas de DataOps, com foco em qualidade, organizaÃ§Ã£o e rastreabilidade dos dados.

---

## ğŸ“Š Fontes de Dados

### ğŸš Ã”nibus â€” Tempo Real

- Origem: Portal de Dados Abertos de Belo Horizonte (CKAN)
- Tipo de acesso: API (`datastore_search`)
- Formato recebido: JSON
- CaracterÃ­stica: dados transacionais em tempo real

### ğŸ—ºï¸ Mapa de Controle Operacional (MCO)

- Origem: Portal de Dados Abertos de Belo Horizonte (CKAN)
- Tipo de acesso: arquivos CSV mensais
- PerÃ­odo utilizado: Janeiro a Dezembro de 2024
- CaracterÃ­stica: dados histÃ³ricos

---

## ğŸ—ï¸ Arquitetura Medallion

O pipeline Ã© estruturado em trÃªs camadas:

### ğŸ¥‰ Bronze

- Armazena dados crus (raw)
- Sem transformaÃ§Ãµes de negÃ³cio
- Formato: Parquet
- OrganizaÃ§Ã£o por fonte e perÃ­odo

### ğŸ¥ˆ Silver

- Limpeza e padronizaÃ§Ã£o de colunas
- Tipagem de dados
- UniÃ£o dos arquivos mensais do MCO
- RemoÃ§Ã£o de duplicados
- ValidaÃ§Ã£o de esquema
- Formato: Delta Lake

### ğŸ¥‡ Gold

- Tabelas agregadas
- MÃ©tricas analÃ­ticas
- Dados prontos para BI e Machine Learning
- Formato: Delta Lake

---

## ğŸ“ Estrutura do Projeto

```
urban-mobility-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ enviroment.yml
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ EstratÃ©gia de IngestÃ£o

### Ã”nibus (Tempo Real)

Fluxo:

```
API CKAN â†’ JSON â†’ Spark DataFrame â†’ Parquet (Bronze)
```

### MCO

Fluxo:

```
Download CSV â†’ Spark DataFrame â†’ Parquet (Bronze)
```

Cada mÃªs do MCO Ã© armazenado separadamente na camada Bronze para preservar granularidade e rastreabilidade.

Nenhuma agregaÃ§Ã£o Ã© realizada nesta etapa.

---

## ğŸš§ Desafios Encontrados e SoluÃ§Ãµes

### 403 Forbidden ao acessar CKAN

- Causa: ausÃªncia de User-Agent nos requests HTTP.
- SoluÃ§Ã£o: inclusÃ£o de header `User-Agent` em todas as requisiÃ§Ãµes.

### Spark nÃ£o lÃª CSV diretamente via HTTPS

- SoluÃ§Ã£o: download local do arquivo antes do processamento pelo Spark.

### Avisos de memÃ³ria do Spark

- Heap padrÃ£o insuficiente (~1GB).
- SoluÃ§Ã£o: configuraÃ§Ã£o explÃ­cita de `spark.driver.memory = 4g`.

### Porta padrÃ£o do SparkUI ocupada

- Spark automaticamente utilizou porta alternativa (4041).

---

## ğŸ§  DecisÃµes TÃ©cnicas

- Spark em modo local para simplificaÃ§Ã£o do ambiente.
- Parquet utilizado na Bronze por eficiÃªncia de armazenamento.
- Delta Lake nas camadas Silver e Gold para controle transacional.
- SeparaÃ§Ã£o clara entre ingestÃ£o e transformaÃ§Ã£o.
- Bronze contÃ©m apenas dados crus.
- TransformaÃ§Ãµes somente a partir da Silver.

---

## âœ… Qualidade de Dados

Planejado para Silver e Gold:

- VerificaÃ§Ã£o de valores nulos
- RemoÃ§Ã£o de duplicados
- ValidaÃ§Ã£o de esquema
- Contagem de registros entre camadas

---

## ğŸ“ˆ Camada Gold (Planejado)

Exemplos de tabelas:

- Quantidade de Ã´nibus por linha
- Volume de registros MCO por mÃªs
- MÃ©tricas temporais
- Dados prontos para dashboards

---

## â±ï¸ OrquestraÃ§Ã£o

Atualmente executado via scripts Python.

Planejado:

- Airflow ou Databricks Workflows

---

## â–¶ï¸ Como Executar

### 1ï¸âƒ£ Criar ambiente Conda

```bash
conda env create -f environment.yml
conda activate urban-pipeline
```

> O `environment.yml` define a versÃ£o do Python e dependÃªncias base do projeto.

---

### 2ï¸âƒ£ Instalar dependÃªncias adicionais (se necessÃ¡rio)

```bash
pip install -r requirements.txt
```

> Utilizado para garantir compatibilidade exata das bibliotecas (Delta Lake, PySpark, etc).

---

### 3ï¸âƒ£ Executar o pipeline completo

```bash
python run_pipeline.py
```

Este comando executa automaticamente:

- Bronze â†’ ingestÃ£o e armazenamento em Parquet
- Silver â†’ limpeza, padronizaÃ§Ã£o e enriquecimento (Delta Lake)
- Gold â†’ agregaÃ§Ãµes analÃ­ticas (Delta Lake)

---

## ğŸ” Reprodutibilidade

Para reproduzir o projeto do zero basta:

```bash
git clone <repo>
cd urban-mobility-pipeline
conda env create -f environment.yml
conda activate urban-pipeline
pip install -r requirements.txt
python run_pipeline.py
```

---

## ğŸ”® Melhorias Futuras

- Carga incremental
- Particionamento por data
- Monitoramento
- Logs estruturados
- Alertas automÃ¡ticos
- Deploy em cloud

---

## ğŸ—ºï¸ Diagrama de Arquitetura

```
CKAN APIs / CSV
        â†“
     Bronze
        â†“
     Silver
        â†“
      Gold
        â†“
     BI / ML
```

---

## ğŸ‘¤ Autor

Paulo Ricardo Dantas  
Projeto desenvolvido como estudo de caso para vaga de Engenheiro de Dados Pleno.

---

# â­ Nota

Este projeto prioriza clareza arquitetural, boas prÃ¡ticas de engenharia de dados e rastreabilidade completa do pipeline.
